{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning-(Basic).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7uOymHDA6cttsL+151Fi6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim-minsol/MachineLearning2021-study/blob/main/DeepLearning_(Basic).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugOpYYW4bymY"
      },
      "source": [
        "Numpy 이용한 인공신경망 데이터 이해"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrA1d1ljXNzD",
        "outputId": "ff3d3ec5-faf3-4475-b23a-e999f20f883a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "1. numpy 라이브러리를 이용하여 아래 속성 정보를 가지는 스칼라 텐서 객체 생성\n",
        "-------- [출력 결과] --------\n",
        "*******1. 스칼라(0D 텐서)*******\n",
        "차원        :  0\n",
        "크기        :  ()\n",
        "데이터 타입 :  int64\n",
        "--------------------------\n",
        "\"\"\"\n",
        "d0_tensor = np.array(2)\n",
        "\n",
        "# 생성한 스칼라 텐서 객체 속성 정보 출력\n",
        "print('*******1. 스칼라(0D 텐서)*******')\n",
        "print('차원        : ', d0_tensor.ndim)\n",
        "print('크기        : ', d0_tensor.shape)\n",
        "print('데이터 타입 : ', d0_tensor.dtype)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "2. numpy 라이브러리를 이용하여 아래 속성 정보를 가지는 백터 텐서 객체 생성\n",
        "-------- [출력 결과] --------\n",
        "*******2. 벡터(1D 텐서)*******\n",
        "차원        :  1\n",
        "크기        :  (5,)\n",
        "데이터 타입 :  float64\n",
        "--------------------------\n",
        "\"\"\"\n",
        "d1_tensor = np.array([1.,2.,3.,4.,5.])\n",
        "\n",
        "# 생성한 벡터 텐서 객체 속성 정보 출력\n",
        "print('\\n*******2. 벡터(1D 텐서)*******')\n",
        "print('차원        : ', d1_tensor.ndim)\n",
        "print('크기        : ', d1_tensor.shape)\n",
        "print('데이터 타입 : ', d1_tensor.dtype)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "3. numpy 라이브러리를 이용하여 아래 속성 정보를 가지는 행렬 텐서 객체 생성\n",
        "-------- [출력 결과] --------\n",
        "*******3. 행렬(2D 텐서)*******\n",
        "차원        :  2\n",
        "크기        :  (3, 2)\n",
        "데이터 타입 :  int64\n",
        "--------------------------\n",
        "\"\"\"\n",
        "d2_tensor = np.array([[1,2],\n",
        "                      [3,4],\n",
        "                      [5,6]])\n",
        "\n",
        "# 생성한 행렬 텐서 객체 속성 정보 출력\n",
        "print('\\n*******3. 행렬(2D 텐서)*******')\n",
        "print('차원        : ', d2_tensor.ndim)\n",
        "print('크기        : ', d2_tensor.shape)\n",
        "print('데이터 타입 : ', d2_tensor.dtype)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "4. numpy 라이브러리를 이용하여 아래 속성 정보를 가지는 3D 텐서 객체 생성\n",
        "-------- [출력 결과] --------\n",
        "*******4. 3D 텐서*******\n",
        "차원        :  3\n",
        "크기        :  (2, 3, 1)\n",
        "데이터 타입 :  int64\n",
        "--------------------------\n",
        "\"\"\"\n",
        "d3_tensor = np.array([[[1],\n",
        "                       [2],\n",
        "                       [3]],\n",
        "                      [[4],\n",
        "                       [5],\n",
        "                       [6]]])\n",
        "\n",
        "# 생성한 3D 텐서 객체 속성 정보 출력\n",
        "print('\\n*******4. 3D 텐서*******')\n",
        "print('차원        : ', d3_tensor.ndim)\n",
        "print('크기        : ', d3_tensor.shape)\n",
        "print('데이터 타입 : ', d3_tensor.dtype)\n",
        "          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*******1. 스칼라(0D 텐서)*******\n",
            "차원        :  0\n",
            "크기        :  ()\n",
            "데이터 타입 :  int64\n",
            "\n",
            "*******2. 벡터(1D 텐서)*******\n",
            "차원        :  1\n",
            "크기        :  (5,)\n",
            "데이터 타입 :  float64\n",
            "\n",
            "*******3. 행렬(2D 텐서)*******\n",
            "차원        :  2\n",
            "크기        :  (3, 2)\n",
            "데이터 타입 :  int64\n",
            "\n",
            "*******4. 3D 텐서*******\n",
            "차원        :  3\n",
            "크기        :  (2, 3, 1)\n",
            "데이터 타입 :  int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpNxJipAh9o0"
      },
      "source": [
        "TensorFlow 2.0 를 이용한 인공신경망 데이터 이해"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IVEzv4Uh9dm",
        "outputId": "9bf0acd0-0f3a-4df8-a6cc-f635a48e5e40"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\"\"\"\n",
        "1. tensorflow 라이브러리를 이용하여 아래 속성 정보를 가지는 상수 텐서 객체 생성 및 속성 정보 출력\n",
        "-------- [출력 결과] --------\n",
        "*******1. 상수 텐서 객체*******\n",
        "차원        :  3\n",
        "크기        :  (3, 1, 2)\n",
        "데이터 타입 :  <dtype: 'int32'>\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "# 상수 텐서 객체 생성\n",
        "d3_tensor = tf.constant([[[1,2]],\n",
        "                         [[4,5]],\n",
        "                         [[6,7]]])\n",
        "\n",
        "# 생성한 상수 텐서 객체 속성 정보 출력\n",
        "print('*******1. 상수 텐서 객체*******')\n",
        "print('차원        : ', d3_tensor.ndim)\n",
        "print('크기        : ', d3_tensor.shape)\n",
        "print('데이터 타입 : ', d3_tensor.dtype)\n",
        "\n",
        "\"\"\"\n",
        "2. 상수 텐서 객체의 값(ndarray) 복사하여 속성 정보 출력\n",
        "-------- [출력 결과] --------\n",
        "*******2. ndarray 객체*******\n",
        "차원        :  3\n",
        "크기        :  (3, 1, 2)\n",
        "데이터 타입 :  int32\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "# 상수 텐서에서 .numpy() 메서드 이용하여 numpy ndarray 객체 복사\n",
        "np_d3_tensor = d3_tensor.numpy()\n",
        "np_d3_tensor = np_d3_tensor.astype(np.int32)\n",
        "\n",
        "# 복사한 ndarray 객체의 속성 정보 출력\n",
        "print('\\n*******2. ndarray 객체*******')\n",
        "print('차원        : ', np_d3_tensor.ndim)\n",
        "print('크기        : ', np_d3_tensor.shape)\n",
        "print('데이터 타입 : ', np_d3_tensor.dtype)\n",
        "\n",
        "\"\"\"\n",
        "3. tensorflow 라이브러리를 이용하여 아래 속성 정보를 가지는 변수 텐서 객체 생성 및 속성 정보 출력\n",
        "-------- [출력 결과] --------\n",
        "*******3. 변수 텐서 객체*******\n",
        "크기        :  (2, 3, 1)\n",
        "데이터 타입 :  <dtype: 'int32'>\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "# 형태의 변수 텐서 객체 생성\n",
        "d3_np_tensor = np.array([[[1],\n",
        "                           [2],\n",
        "                           [3]],\n",
        "                          [[4],\n",
        "                           [5],\n",
        "                           [6]]])\n",
        "d3_np_tensor = d3_np_tensor.astype(np.int32)\n",
        "\n",
        "d3_var_tensor = tf.Variable(d3_np_tensor)\n",
        "\n",
        "print('\\n*******3. 변수 텐서 객체*******')\n",
        "print('크기        : ', d3_var_tensor.shape)\n",
        "print('데이터 타입 : ', d3_var_tensor.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*******1. 상수 텐서 객체*******\n",
            "차원        :  3\n",
            "크기        :  (3, 1, 2)\n",
            "데이터 타입 :  <dtype: 'int32'>\n",
            "\n",
            "*******2. ndarray 객체*******\n",
            "차원        :  3\n",
            "크기        :  (3, 1, 2)\n",
            "데이터 타입 :  int32\n",
            "\n",
            "*******3. 변수 텐서 객체*******\n",
            "크기        :  (2, 3, 1)\n",
            "데이터 타입 :  <dtype: 'int32'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oztyGjHd56P3"
      },
      "source": [
        "Tensorflow 연산자 이해"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCrMkyoM56HI",
        "outputId": "ca316e6f-9dd8-4900-bc6a-82e81a2487af"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# (2, 3) 형태의 상수 텐서 'tensor_a' 생성\n",
        "tensor_a = tf.constant([[1., 2., 3.],\n",
        "                        [4., 5., 6.]])\n",
        "\n",
        "\"\"\"\n",
        "1. tensorflow 연산자를 이용해 'tensor_a' 텐서에 지수 연산을 적용\n",
        "-------- [출력 결과] --------\n",
        "*******1. 지수연산*******\n",
        "[[ *.** *.** *.** ]\n",
        " [ *.** *.** *.** ]]\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "# tf.math 모듈의 지수(exponential)함수를 이용해 'tensor_a' 텐서에 지수 연산 적용\n",
        "print('*******1. 지수연산*******')\n",
        "\n",
        "tensor_exp = tf.math.square(tensor_a)\n",
        "print(np.round(tensor_exp, 2))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "2. tensorflow 연산자를 이용해 'tensor_a'의 axis 0 을 기준으로 최대값 선택\n",
        "-------- [출력 결과] --------\n",
        "*******2. axis 0 기준 최대값*******\n",
        "[ * * ]\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "# tf.math 모듈의 함수를 이용해 'tensor_a'의 axis 0 기준 최대값 선택\n",
        "print('\\n*******2. axis 0 기준 최대값*******')\n",
        "\n",
        "tensor_axis_max = tensor_a.numpy()\n",
        "tensor_axis_max = tensor_axis_max.max(axis=0)\n",
        "\n",
        "print(tensor_axis_max)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "3. tensorflow 연산자를 이용해 z = (x+y)^2 계산식에서 x 값이 z 값에 미치는 영향도(미분) 계산\n",
        "-------- [출력 결과] --------\n",
        "*******3. 'tensor_x' 값이 'tensor_z' 값에 미치는 영향도(미분 값)*******\n",
        "[[*.**  *.**  *.**]\n",
        " [*.**  *.**  *.**]]\n",
        "--------------------------\n",
        "\"\"\"\n",
        "\n",
        "tensor_x = tf.Variable([[1., 2., 3.],\n",
        "                        [4., 5., 6.]])\n",
        "\n",
        "tensor_y = tf.Variable([[7., 8., 9.],\n",
        "                        [10., 11., 12.]])\n",
        "\n",
        "print(\"\\n*******3. 'tensor_x' 값이 'tensor_z' 값에 미치는 영향도(미분 값)*******\")\n",
        "with tf.GradientTape() as tape:\n",
        "  # 텐서플로 연산자와 변수 텐서를 이용해 계산그래프 정의\n",
        "  tensor_z = tf.square(tensor_x + tensor_y)\n",
        "  \n",
        "  # tape.gradient를 통해 tensor_x 가 tensor_z 에 미치는 영향도(미분 값)를 계산\n",
        "  gradient_zx = tape.gradient(target=tensor_z, sources=tensor_x)\n",
        "  \n",
        "  # 계산된 영향도(미분 값) 확인\n",
        "  print(np.round(gradient_zx, 2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*******1. 지수연산*******\n",
            "[[ 1.  4.  9.]\n",
            " [16. 25. 36.]]\n",
            "\n",
            "*******2. axis 0 기준 최대값*******\n",
            "[4. 5. 6.]\n",
            "\n",
            "*******3. 'tensor_x' 값이 'tensor_z' 값에 미치는 영향도(미분 값)*******\n",
            "[[16. 20. 24.]\n",
            " [28. 32. 36.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlLDNNO2EB2l"
      },
      "source": [
        "실습 : TensorFlow 2.0 을 이용한 퍼셉트론 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at8FxelnEBsA",
        "outputId": "c28241aa-5dce-46e9-9440-a034e83590f2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 파라미터 값(input 특성 개수, 퍼셉트론의 개수, 학습률) 설정\n",
        "input_dim = 2\n",
        "hidden_units = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "# 가중치(input 특성 : 2/ 퍼셉트론 : 1)\n",
        "w = tf.Variable(tf.random.uniform(shape=(input_dim, hidden_units)))\n",
        "\n",
        "# 편향(퍼셉트론 : 1)\n",
        "b = tf.Variable(tf.zeros(shape=(hidden_units,)))\n",
        "\n",
        "# 설계한 퍼셉트론 모델의 파라미터 확인(코드 제출시 주석 처리)\n",
        "#print('*******퍼셉트론 모델의 초기 가중치*******')\n",
        "#print(w)\n",
        "#print('\\n*******퍼셉트론 모델의 초기 편향*******')\n",
        "#print(b)\n",
        "\n",
        "# 퍼셉트론의 수학 모델 f(x*w + b) 구현\n",
        "def predict(input):\n",
        "  # x*w + b 연산 구현\n",
        "  x = tf.matmul(input, w) + b\n",
        "  # relu 활성화 함수 구현\n",
        "  x = tf.maximum(0, x)\n",
        "  return x\n",
        "\n",
        "# loss(mse)\n",
        "def mse_loss(labels, predictions):\n",
        "  # MSE(Mean Square Error) 연산 구현\n",
        "  loss = tf.reduce_mean(tf.square(labels - predictions))\n",
        "  return loss\n",
        "\n",
        "# train\n",
        "def train(inputs, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # 퍼셉트론 모델을 예측값을 계산\n",
        "    predictions = predict(inputs)\n",
        "    # 모델의 예측값과 정답간의 에러를 loss 을 이용해 계산\n",
        "    loss = mse_loss(labels, predictions)\n",
        "    # 모델의 파라미터(w, b)가 loss 값에 미치는 영향도를 미분(오차역전파)을 통해 계산\n",
        "    gradient_lw, gradient_lb = tape.gradient(loss,[w,b])\n",
        "  # 경사하강법을 수행하여 모델의 파라미터(w, b) 업데이트\n",
        "  w.assign(w-learning_rate * gradient_lw)\n",
        "  b.assign(b-learning_rate * gradient_lb)\n",
        "  return loss\n",
        "\n",
        "# 퍼셉트론 모델 학습을 위한 OR Gate 데이터 생성\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]], dtype=np.float32)\n",
        "\n",
        "labels = np.array([0, 1, 1, 1], dtype=np.float32)\n",
        "\n",
        "# OR Gate 데이터를 차트로 확인(코드 제출시 주석 처리)\n",
        "#plt.scatter(inputs[:, 0], inputs[:, 1], c=labels[:])\n",
        "#plt.show()\n",
        "\n",
        "# train 함수를 반복적으로 실행하여 퍼셉트론 모델을 학습\n",
        "for epoch in range(100):\n",
        "  # input 데이터와 label 데이터를 한 건씩 추출하여 train 함수에 전달\n",
        "  for x, y in zip(inputs, labels):\n",
        "    loss = train([x], [y])\n",
        "  # 학습 중간에 loss 값의 변화 출력(코드 제출시 주석 처리)\n",
        "  #if (epoch+1)%10 == 0:\n",
        "    #print(\"Epoch {}: loss={}\".format(epoch+1, float(loss)))\n",
        "\n",
        "# 학습된 모델에 input 데이터 입력하여 예측결과 계산\n",
        "predictions = predict(inputs)\n",
        "\n",
        "# 모델의 예측결과를 차트로 확인(코드 제출시 주석 처리)\n",
        "#plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:]> 0.5)\n",
        "#plt.show()\n",
        "\n",
        "print('*******모델의 예측 결과*******')\n",
        "print(predictions[:]> 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*******모델의 예측 결과*******\n",
            "tf.Tensor(\n",
            "[[False]\n",
            " [ True]\n",
            " [ True]\n",
            " [ True]], shape=(4, 1), dtype=bool)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlA0LPSZvXq8"
      },
      "source": [
        "Keras API 를 활용한 fashion-mnist 데이터셋 분류 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "swuA3nAqvXZb",
        "outputId": "b78bf00f-e7c8-48c4-cada-284c194dcc90"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"* Step 1. Input tensor 와 Target tensor 준비(훈련데이터)\"\"\"\n",
        "\n",
        "# label 데이터의 각 value 에 해당하는 class name 정보\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. import 한 fashion_mnist API를 이용하여 fashion_mnist 데이터 셋을 다운로드 받으세요\n",
        "(train_images, train_labels), (test_images, test_labels)=fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "\"\"\"* Step 2. 데이터의 전처리 \"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. 3차원 형태(batch, hight, width)의 train, test feature 데이터를 2차원(batch, hight*width)으로 변경 하세요\n",
        "# 2. feature 데이터를 [0, 1] 사이로 scailing을 수행하세요\n",
        "train_images = train_images.reshape(60000, 784)\n",
        "test_images = test_images.reshape(10000, 784)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mms = MinMaxScaler()\n",
        "mms.fit(train_images)\n",
        "train_images=mms.transform(train_images)\n",
        "\n",
        "mms = MinMaxScaler()\n",
        "mms.fit(test_images)\n",
        "test_images=mms.transform(test_images)\n",
        "\n",
        "\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. 1차원 형태의(batch, ) class index 인 train, test label 데이터를\n",
        "#    to_categorical API를 이용하여 one-hot-encoding 수행하여 2차원(batch, class_cnt) 으로 변경 하세요\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* Step 3. DNN(MLP) 모델 디자인\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. Sequential API를 이용하여 fashion_mnist 데이터 셋을 분석 하기 위한 MLP 모델을 디자인 하세요\n",
        "from tensorflow.keras import models, layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(units=160, activation = 'relu', input_shape=(28*28,)))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\"\"\"* Step 4. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정하세요\n",
        "#   - optimizer\n",
        "#   - loss : categorical_crossentropy 로 설정(label 데이터를 one-hot-encoding 하였기 때문)\n",
        "#   - metrics : 체점 기준인 accuracy 로 설정\n",
        "\n",
        "model.compile(optimizer = 'rmsprop',\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "\"\"\"* Step 5. 모델에 input, target 데이터 연결 후 학습\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습하세요\n",
        "#   - fit 메서드의 verbose=2 로 설정 하세요\n",
        "\n",
        "model.fit(x = train_images,\n",
        "          y = train_labels,\n",
        "          epochs = 30,\n",
        "          batch_size = 40,\n",
        "          validation_split = 0.2,\n",
        "          verbose = 2)\n",
        "\n",
        "\n",
        "\"\"\"* 모델 제출 \"\"\"\n",
        "\n",
        "# 학습된 모델을 제출하기 위한 코드 입니다. 수정하지 마세요\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 160)               125600    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1610      \n",
            "=================================================================\n",
            "Total params: 127,210\n",
            "Trainable params: 127,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "1200/1200 - 4s - loss: 0.5308 - accuracy: 0.8102 - val_loss: 0.4020 - val_accuracy: 0.8602\n",
            "Epoch 2/30\n",
            "1200/1200 - 3s - loss: 0.3862 - accuracy: 0.8611 - val_loss: 0.3881 - val_accuracy: 0.8658\n",
            "Epoch 3/30\n",
            "1200/1200 - 3s - loss: 0.3458 - accuracy: 0.8764 - val_loss: 0.3761 - val_accuracy: 0.8753\n",
            "Epoch 4/30\n",
            "1200/1200 - 3s - loss: 0.3267 - accuracy: 0.8828 - val_loss: 0.3620 - val_accuracy: 0.8749\n",
            "Epoch 5/30\n",
            "1200/1200 - 3s - loss: 0.3097 - accuracy: 0.8886 - val_loss: 0.3446 - val_accuracy: 0.8758\n",
            "Epoch 6/30\n",
            "1200/1200 - 3s - loss: 0.2955 - accuracy: 0.8943 - val_loss: 0.3703 - val_accuracy: 0.8792\n",
            "Epoch 7/30\n",
            "1200/1200 - 3s - loss: 0.2867 - accuracy: 0.8983 - val_loss: 0.3738 - val_accuracy: 0.8783\n",
            "Epoch 8/30\n",
            "1200/1200 - 3s - loss: 0.2790 - accuracy: 0.9013 - val_loss: 0.3945 - val_accuracy: 0.8754\n",
            "Epoch 9/30\n",
            "1200/1200 - 3s - loss: 0.2722 - accuracy: 0.9031 - val_loss: 0.3916 - val_accuracy: 0.8798\n",
            "Epoch 10/30\n",
            "1200/1200 - 3s - loss: 0.2655 - accuracy: 0.9057 - val_loss: 0.3870 - val_accuracy: 0.8824\n",
            "Epoch 11/30\n",
            "1200/1200 - 3s - loss: 0.2602 - accuracy: 0.9076 - val_loss: 0.3783 - val_accuracy: 0.8871\n",
            "Epoch 12/30\n",
            "1200/1200 - 3s - loss: 0.2532 - accuracy: 0.9102 - val_loss: 0.3878 - val_accuracy: 0.8883\n",
            "Epoch 13/30\n",
            "1200/1200 - 3s - loss: 0.2456 - accuracy: 0.9134 - val_loss: 0.3651 - val_accuracy: 0.8903\n",
            "Epoch 14/30\n",
            "1200/1200 - 3s - loss: 0.2401 - accuracy: 0.9160 - val_loss: 0.4255 - val_accuracy: 0.8698\n",
            "Epoch 15/30\n",
            "1200/1200 - 3s - loss: 0.2337 - accuracy: 0.9177 - val_loss: 0.4249 - val_accuracy: 0.8823\n",
            "Epoch 16/30\n",
            "1200/1200 - 3s - loss: 0.2282 - accuracy: 0.9194 - val_loss: 0.4323 - val_accuracy: 0.8867\n",
            "Epoch 17/30\n",
            "1200/1200 - 3s - loss: 0.2278 - accuracy: 0.9211 - val_loss: 0.4437 - val_accuracy: 0.8875\n",
            "Epoch 18/30\n",
            "1200/1200 - 3s - loss: 0.2203 - accuracy: 0.9232 - val_loss: 0.4733 - val_accuracy: 0.8803\n",
            "Epoch 19/30\n",
            "1200/1200 - 3s - loss: 0.2182 - accuracy: 0.9251 - val_loss: 0.4496 - val_accuracy: 0.8887\n",
            "Epoch 20/30\n",
            "1200/1200 - 3s - loss: 0.2129 - accuracy: 0.9265 - val_loss: 0.4208 - val_accuracy: 0.8876\n",
            "Epoch 21/30\n",
            "1200/1200 - 3s - loss: 0.2077 - accuracy: 0.9287 - val_loss: 0.4651 - val_accuracy: 0.8856\n",
            "Epoch 22/30\n",
            "1200/1200 - 3s - loss: 0.2023 - accuracy: 0.9302 - val_loss: 0.4980 - val_accuracy: 0.8798\n",
            "Epoch 23/30\n",
            "1200/1200 - 3s - loss: 0.1989 - accuracy: 0.9319 - val_loss: 0.4741 - val_accuracy: 0.8875\n",
            "Epoch 24/30\n",
            "1200/1200 - 3s - loss: 0.1963 - accuracy: 0.9337 - val_loss: 0.4832 - val_accuracy: 0.8835\n",
            "Epoch 25/30\n",
            "1200/1200 - 3s - loss: 0.1917 - accuracy: 0.9340 - val_loss: 0.4824 - val_accuracy: 0.8804\n",
            "Epoch 26/30\n",
            "1200/1200 - 3s - loss: 0.1876 - accuracy: 0.9359 - val_loss: 0.5052 - val_accuracy: 0.8888\n",
            "Epoch 27/30\n",
            "1200/1200 - 3s - loss: 0.1853 - accuracy: 0.9356 - val_loss: 0.5067 - val_accuracy: 0.8894\n",
            "Epoch 28/30\n",
            "1200/1200 - 3s - loss: 0.1835 - accuracy: 0.9376 - val_loss: 0.4887 - val_accuracy: 0.8899\n",
            "Epoch 29/30\n",
            "1200/1200 - 3s - loss: 0.1785 - accuracy: 0.9395 - val_loss: 0.5158 - val_accuracy: 0.8881\n",
            "Epoch 30/30\n",
            "1200/1200 - 3s - loss: 0.1755 - accuracy: 0.9404 - val_loss: 0.4983 - val_accuracy: 0.8942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MmM20NuDEhd"
      },
      "source": [
        "자동차 연비 데이터셋 파일 다운"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3F7c2S5DDjO"
      },
      "source": [
        "auto_mpg_dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
        "dataset_path = tf.keras.utils.get_file(\"/content/auto-mpg.data\", auto_mpg_dataset_url)\n",
        "dataset_path\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM8ulDQrbPXi"
      },
      "source": [
        "Keras API를 이용한 보스턴 집값 회귀 실습_검증코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaPHeQR-bPLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a6fc170-6052-4b08-e807-80087d7b9716"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "from tensorflow.keras import models,layers\n",
        "import pandas as pd\n",
        "\n",
        "\"\"\"* Step 1. Input tensor 와 Target tensor 준비(훈련데이터)\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. import 한 boston_housing API를 이용하여 boston_housing 데이터 셋을 다운로드 받으세요\n",
        "(train_dataset, train_labels), (test_dataset, test_labels)= boston_housing.load_data()\n",
        "\n",
        "columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
        "         'B', 'LSTAT']\n",
        "\n",
        "train_dataset = pd.DataFrame(train_dataset, columns=columns)\n",
        "#print(train_dataset)\n",
        "\n",
        "#print(train_dataset.isna().sum())\n",
        "\n",
        "#print(train_dataset.describe())\n",
        "\n",
        "dataset = train_dataset.describe()\n",
        "usu = dataset.transpose()\n",
        "\n",
        "#print(usu)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* Step 2. 데이터의 전처리 \"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. train 데이터의 feature 별 평균값, 표준편차 값을 이용하여 정규화 작업을 진행하세요\n",
        "\n",
        "def norm(x):\n",
        "  normed_data = (x - usu['mean']) / usu['std']\n",
        "  return normed_data\n",
        "\n",
        "train_dataset = norm(train_dataset)\n",
        "\n",
        "#print(train_dataset)\n",
        "\n",
        "\"\"\"* Step 3. DNN(MLP) 모델 디자인\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. Sequential API를 이용하여 boston_housing 데이터 셋을 분석 하기 위한 MLP 모델을 디자인 하세요\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(units=64, activation = 'relu', input_shape=(len(train_dataset.keys()) ,)))\n",
        "model.add(layers.Dense(units=32, activation = 'relu'))\n",
        "model.add(layers.Dense(units=1))\n",
        "\n",
        "\n",
        "\"\"\"* Step 4. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정하세요\n",
        "#   - optimizer\n",
        "#   - loss\n",
        "#   - metrics : 체점 기준인 MAE 로 설정\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer = 'rmsprop',\n",
        "              metrics = ['mae'])\n",
        "\n",
        "\n",
        "\"\"\"* Step 5. 모델에 input, target 데이터 연결 후 학습\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습하세요\n",
        "#   - fit 메서드의 verbose=2 로 설정 하세요\n",
        "model.fit(x=train_dataset, y= train_labels,\n",
        "          batch_size=16,\n",
        "          epochs=100,\n",
        "          validation_split = 0.2,\n",
        "          verbose=2)\n",
        "\n",
        "\n",
        "\"\"\"* 모델 제출 \"\"\"\n",
        "\n",
        "# 학습된 모델을 제출하기 위한 코드 입니다. 수정하지 마세요\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "Epoch 1/100\n",
            "21/21 - 1s - loss: 491.3745 - mae: 20.2228 - val_loss: 500.0025 - val_mae: 20.4300\n",
            "Epoch 2/100\n",
            "21/21 - 0s - loss: 375.2428 - mae: 17.2460 - val_loss: 369.7626 - val_mae: 17.1601\n",
            "Epoch 3/100\n",
            "21/21 - 0s - loss: 246.8032 - mae: 13.5048 - val_loss: 230.5961 - val_mae: 12.7227\n",
            "Epoch 4/100\n",
            "21/21 - 0s - loss: 132.8817 - mae: 9.2962 - val_loss: 127.1759 - val_mae: 8.6413\n",
            "Epoch 5/100\n",
            "21/21 - 0s - loss: 73.9208 - mae: 6.5032 - val_loss: 84.5669 - val_mae: 6.8867\n",
            "Epoch 6/100\n",
            "21/21 - 0s - loss: 51.3544 - mae: 5.1812 - val_loss: 63.2429 - val_mae: 5.8507\n",
            "Epoch 7/100\n",
            "21/21 - 0s - loss: 39.6361 - mae: 4.4010 - val_loss: 47.2416 - val_mae: 5.0540\n",
            "Epoch 8/100\n",
            "21/21 - 0s - loss: 31.7911 - mae: 3.7946 - val_loss: 37.2993 - val_mae: 4.4635\n",
            "Epoch 9/100\n",
            "21/21 - 0s - loss: 27.1076 - mae: 3.4341 - val_loss: 29.9961 - val_mae: 4.0149\n",
            "Epoch 10/100\n",
            "21/21 - 0s - loss: 23.8531 - mae: 3.2191 - val_loss: 25.9825 - val_mae: 3.7568\n",
            "Epoch 11/100\n",
            "21/21 - 0s - loss: 21.5439 - mae: 3.0737 - val_loss: 22.7314 - val_mae: 3.5287\n",
            "Epoch 12/100\n",
            "21/21 - 0s - loss: 19.6288 - mae: 2.9844 - val_loss: 20.7055 - val_mae: 3.4596\n",
            "Epoch 13/100\n",
            "21/21 - 0s - loss: 18.1800 - mae: 2.8757 - val_loss: 18.3402 - val_mae: 3.1823\n",
            "Epoch 14/100\n",
            "21/21 - 0s - loss: 17.0933 - mae: 2.7625 - val_loss: 17.1551 - val_mae: 3.0618\n",
            "Epoch 15/100\n",
            "21/21 - 0s - loss: 15.7813 - mae: 2.6666 - val_loss: 15.7280 - val_mae: 2.9491\n",
            "Epoch 16/100\n",
            "21/21 - 0s - loss: 14.8791 - mae: 2.5773 - val_loss: 15.0558 - val_mae: 2.9030\n",
            "Epoch 17/100\n",
            "21/21 - 0s - loss: 13.9050 - mae: 2.5088 - val_loss: 14.4529 - val_mae: 2.9100\n",
            "Epoch 18/100\n",
            "21/21 - 0s - loss: 13.2757 - mae: 2.4549 - val_loss: 14.2203 - val_mae: 2.9040\n",
            "Epoch 19/100\n",
            "21/21 - 0s - loss: 12.4657 - mae: 2.3910 - val_loss: 13.2621 - val_mae: 2.8293\n",
            "Epoch 20/100\n",
            "21/21 - 0s - loss: 12.0077 - mae: 2.3197 - val_loss: 12.5058 - val_mae: 2.7014\n",
            "Epoch 21/100\n",
            "21/21 - 0s - loss: 11.7194 - mae: 2.2965 - val_loss: 12.2264 - val_mae: 2.6788\n",
            "Epoch 22/100\n",
            "21/21 - 0s - loss: 11.2419 - mae: 2.2466 - val_loss: 12.2325 - val_mae: 2.6995\n",
            "Epoch 23/100\n",
            "21/21 - 0s - loss: 10.9797 - mae: 2.2128 - val_loss: 11.7108 - val_mae: 2.5927\n",
            "Epoch 24/100\n",
            "21/21 - 0s - loss: 10.4569 - mae: 2.2067 - val_loss: 11.6874 - val_mae: 2.6139\n",
            "Epoch 25/100\n",
            "21/21 - 0s - loss: 10.1767 - mae: 2.1628 - val_loss: 11.5359 - val_mae: 2.6772\n",
            "Epoch 26/100\n",
            "21/21 - 0s - loss: 10.0499 - mae: 2.1247 - val_loss: 11.3707 - val_mae: 2.5400\n",
            "Epoch 27/100\n",
            "21/21 - 0s - loss: 9.7201 - mae: 2.1531 - val_loss: 12.5632 - val_mae: 2.7577\n",
            "Epoch 28/100\n",
            "21/21 - 0s - loss: 9.4233 - mae: 2.1153 - val_loss: 11.7672 - val_mae: 2.5660\n",
            "Epoch 29/100\n",
            "21/21 - 0s - loss: 9.3602 - mae: 2.0787 - val_loss: 11.2981 - val_mae: 2.5322\n",
            "Epoch 30/100\n",
            "21/21 - 0s - loss: 9.2169 - mae: 2.0665 - val_loss: 12.8817 - val_mae: 2.6909\n",
            "Epoch 31/100\n",
            "21/21 - 0s - loss: 9.0185 - mae: 2.0555 - val_loss: 11.8075 - val_mae: 2.4875\n",
            "Epoch 32/100\n",
            "21/21 - 0s - loss: 9.0070 - mae: 2.0811 - val_loss: 10.9066 - val_mae: 2.5214\n",
            "Epoch 33/100\n",
            "21/21 - 0s - loss: 8.7451 - mae: 2.0223 - val_loss: 11.1371 - val_mae: 2.5031\n",
            "Epoch 34/100\n",
            "21/21 - 0s - loss: 8.5919 - mae: 1.9898 - val_loss: 11.8826 - val_mae: 2.5374\n",
            "Epoch 35/100\n",
            "21/21 - 0s - loss: 8.4430 - mae: 2.0175 - val_loss: 11.1388 - val_mae: 2.4906\n",
            "Epoch 36/100\n",
            "21/21 - 0s - loss: 8.5466 - mae: 2.0235 - val_loss: 10.9663 - val_mae: 2.4554\n",
            "Epoch 37/100\n",
            "21/21 - 0s - loss: 8.1486 - mae: 1.9876 - val_loss: 11.4007 - val_mae: 2.6423\n",
            "Epoch 38/100\n",
            "21/21 - 0s - loss: 8.3463 - mae: 1.9415 - val_loss: 11.3199 - val_mae: 2.4333\n",
            "Epoch 39/100\n",
            "21/21 - 0s - loss: 8.2217 - mae: 1.9659 - val_loss: 11.2141 - val_mae: 2.4718\n",
            "Epoch 40/100\n",
            "21/21 - 0s - loss: 8.1672 - mae: 1.9402 - val_loss: 11.8937 - val_mae: 2.4715\n",
            "Epoch 41/100\n",
            "21/21 - 0s - loss: 8.0180 - mae: 1.9519 - val_loss: 11.3956 - val_mae: 2.5211\n",
            "Epoch 42/100\n",
            "21/21 - 0s - loss: 7.7419 - mae: 1.8959 - val_loss: 13.6650 - val_mae: 2.6195\n",
            "Epoch 43/100\n",
            "21/21 - 0s - loss: 7.9718 - mae: 1.9828 - val_loss: 12.0229 - val_mae: 2.4665\n",
            "Epoch 44/100\n",
            "21/21 - 0s - loss: 7.6729 - mae: 1.9168 - val_loss: 11.6416 - val_mae: 2.4365\n",
            "Epoch 45/100\n",
            "21/21 - 0s - loss: 7.6941 - mae: 1.9056 - val_loss: 11.4864 - val_mae: 2.4219\n",
            "Epoch 46/100\n",
            "21/21 - 0s - loss: 7.4926 - mae: 1.8769 - val_loss: 11.0085 - val_mae: 2.5108\n",
            "Epoch 47/100\n",
            "21/21 - 0s - loss: 7.6787 - mae: 1.8827 - val_loss: 11.4308 - val_mae: 2.4497\n",
            "Epoch 48/100\n",
            "21/21 - 0s - loss: 7.5960 - mae: 1.8881 - val_loss: 11.0148 - val_mae: 2.4627\n",
            "Epoch 49/100\n",
            "21/21 - 0s - loss: 7.3537 - mae: 1.8393 - val_loss: 11.7665 - val_mae: 2.4095\n",
            "Epoch 50/100\n",
            "21/21 - 0s - loss: 7.3014 - mae: 1.8558 - val_loss: 11.5741 - val_mae: 2.5913\n",
            "Epoch 51/100\n",
            "21/21 - 0s - loss: 7.2134 - mae: 1.8200 - val_loss: 11.2043 - val_mae: 2.4184\n",
            "Epoch 52/100\n",
            "21/21 - 0s - loss: 7.2080 - mae: 1.8080 - val_loss: 12.9831 - val_mae: 2.7110\n",
            "Epoch 53/100\n",
            "21/21 - 0s - loss: 7.2142 - mae: 1.8387 - val_loss: 12.1380 - val_mae: 2.5840\n",
            "Epoch 54/100\n",
            "21/21 - 0s - loss: 7.0726 - mae: 1.8036 - val_loss: 11.3755 - val_mae: 2.4712\n",
            "Epoch 55/100\n",
            "21/21 - 0s - loss: 7.1952 - mae: 1.8326 - val_loss: 11.0768 - val_mae: 2.4092\n",
            "Epoch 56/100\n",
            "21/21 - 0s - loss: 7.1464 - mae: 1.7984 - val_loss: 11.6016 - val_mae: 2.4978\n",
            "Epoch 57/100\n",
            "21/21 - 0s - loss: 7.0596 - mae: 1.7967 - val_loss: 11.2673 - val_mae: 2.3972\n",
            "Epoch 58/100\n",
            "21/21 - 0s - loss: 6.8181 - mae: 1.7805 - val_loss: 11.3618 - val_mae: 2.4161\n",
            "Epoch 59/100\n",
            "21/21 - 0s - loss: 6.8349 - mae: 1.7763 - val_loss: 11.3862 - val_mae: 2.4504\n",
            "Epoch 60/100\n",
            "21/21 - 0s - loss: 6.7999 - mae: 1.7668 - val_loss: 11.3672 - val_mae: 2.3908\n",
            "Epoch 61/100\n",
            "21/21 - 0s - loss: 6.7028 - mae: 1.8081 - val_loss: 11.3873 - val_mae: 2.4607\n",
            "Epoch 62/100\n",
            "21/21 - 0s - loss: 6.5641 - mae: 1.7663 - val_loss: 11.1089 - val_mae: 2.4558\n",
            "Epoch 63/100\n",
            "21/21 - 0s - loss: 6.6994 - mae: 1.7654 - val_loss: 10.9070 - val_mae: 2.4257\n",
            "Epoch 64/100\n",
            "21/21 - 0s - loss: 6.6750 - mae: 1.7600 - val_loss: 11.0643 - val_mae: 2.3897\n",
            "Epoch 65/100\n",
            "21/21 - 0s - loss: 6.6304 - mae: 1.7342 - val_loss: 12.3922 - val_mae: 2.5486\n",
            "Epoch 66/100\n",
            "21/21 - 0s - loss: 6.6250 - mae: 1.7400 - val_loss: 10.9067 - val_mae: 2.4721\n",
            "Epoch 67/100\n",
            "21/21 - 0s - loss: 6.3483 - mae: 1.7302 - val_loss: 11.2059 - val_mae: 2.4846\n",
            "Epoch 68/100\n",
            "21/21 - 0s - loss: 6.4387 - mae: 1.7100 - val_loss: 11.8693 - val_mae: 2.3896\n",
            "Epoch 69/100\n",
            "21/21 - 0s - loss: 6.3813 - mae: 1.7262 - val_loss: 11.1351 - val_mae: 2.3546\n",
            "Epoch 70/100\n",
            "21/21 - 0s - loss: 6.0961 - mae: 1.7267 - val_loss: 11.2296 - val_mae: 2.5221\n",
            "Epoch 71/100\n",
            "21/21 - 0s - loss: 6.2222 - mae: 1.6752 - val_loss: 11.6507 - val_mae: 2.4802\n",
            "Epoch 72/100\n",
            "21/21 - 0s - loss: 6.2228 - mae: 1.6731 - val_loss: 11.3441 - val_mae: 2.4843\n",
            "Epoch 73/100\n",
            "21/21 - 0s - loss: 6.1044 - mae: 1.6598 - val_loss: 11.8807 - val_mae: 2.3874\n",
            "Epoch 74/100\n",
            "21/21 - 0s - loss: 6.3484 - mae: 1.7190 - val_loss: 11.1923 - val_mae: 2.4119\n",
            "Epoch 75/100\n",
            "21/21 - 0s - loss: 6.1599 - mae: 1.6997 - val_loss: 11.1952 - val_mae: 2.3800\n",
            "Epoch 76/100\n",
            "21/21 - 0s - loss: 6.1489 - mae: 1.6846 - val_loss: 11.8770 - val_mae: 2.5137\n",
            "Epoch 77/100\n",
            "21/21 - 0s - loss: 6.0536 - mae: 1.6751 - val_loss: 11.8537 - val_mae: 2.4157\n",
            "Epoch 78/100\n",
            "21/21 - 0s - loss: 6.1017 - mae: 1.6931 - val_loss: 12.6957 - val_mae: 2.5008\n",
            "Epoch 79/100\n",
            "21/21 - 0s - loss: 5.9382 - mae: 1.6555 - val_loss: 11.2106 - val_mae: 2.3605\n",
            "Epoch 80/100\n",
            "21/21 - 0s - loss: 6.1069 - mae: 1.6587 - val_loss: 11.4281 - val_mae: 2.3968\n",
            "Epoch 81/100\n",
            "21/21 - 0s - loss: 6.0345 - mae: 1.6823 - val_loss: 11.5546 - val_mae: 2.3777\n",
            "Epoch 82/100\n",
            "21/21 - 0s - loss: 5.7246 - mae: 1.6334 - val_loss: 11.1832 - val_mae: 2.3719\n",
            "Epoch 83/100\n",
            "21/21 - 0s - loss: 5.9629 - mae: 1.6401 - val_loss: 11.2730 - val_mae: 2.3787\n",
            "Epoch 84/100\n",
            "21/21 - 0s - loss: 5.8327 - mae: 1.6503 - val_loss: 12.0817 - val_mae: 2.4260\n",
            "Epoch 85/100\n",
            "21/21 - 0s - loss: 5.7671 - mae: 1.6006 - val_loss: 11.2041 - val_mae: 2.3634\n",
            "Epoch 86/100\n",
            "21/21 - 0s - loss: 5.9491 - mae: 1.6513 - val_loss: 11.7044 - val_mae: 2.3788\n",
            "Epoch 87/100\n",
            "21/21 - 0s - loss: 5.8368 - mae: 1.6556 - val_loss: 11.5149 - val_mae: 2.4429\n",
            "Epoch 88/100\n",
            "21/21 - 0s - loss: 5.8010 - mae: 1.6378 - val_loss: 10.9317 - val_mae: 2.4472\n",
            "Epoch 89/100\n",
            "21/21 - 0s - loss: 5.6467 - mae: 1.5930 - val_loss: 12.5692 - val_mae: 2.4435\n",
            "Epoch 90/100\n",
            "21/21 - 0s - loss: 5.6370 - mae: 1.6206 - val_loss: 11.3431 - val_mae: 2.4147\n",
            "Epoch 91/100\n",
            "21/21 - 0s - loss: 5.5074 - mae: 1.5898 - val_loss: 12.7392 - val_mae: 2.4833\n",
            "Epoch 92/100\n",
            "21/21 - 0s - loss: 5.6780 - mae: 1.6230 - val_loss: 11.1786 - val_mae: 2.4222\n",
            "Epoch 93/100\n",
            "21/21 - 0s - loss: 5.5607 - mae: 1.5752 - val_loss: 12.1263 - val_mae: 2.4444\n",
            "Epoch 94/100\n",
            "21/21 - 0s - loss: 5.6840 - mae: 1.6114 - val_loss: 11.0213 - val_mae: 2.4042\n",
            "Epoch 95/100\n",
            "21/21 - 0s - loss: 5.5328 - mae: 1.5895 - val_loss: 11.3021 - val_mae: 2.3772\n",
            "Epoch 96/100\n",
            "21/21 - 0s - loss: 5.3941 - mae: 1.5579 - val_loss: 13.6116 - val_mae: 2.5464\n",
            "Epoch 97/100\n",
            "21/21 - 0s - loss: 5.6637 - mae: 1.6126 - val_loss: 12.0214 - val_mae: 2.4117\n",
            "Epoch 98/100\n",
            "21/21 - 0s - loss: 5.3189 - mae: 1.5656 - val_loss: 11.1802 - val_mae: 2.3730\n",
            "Epoch 99/100\n",
            "21/21 - 0s - loss: 5.3591 - mae: 1.5677 - val_loss: 11.0417 - val_mae: 2.4102\n",
            "Epoch 100/100\n",
            "21/21 - 0s - loss: 5.3160 - mae: 1.5724 - val_loss: 10.7183 - val_mae: 2.4090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z-thgKseXpB"
      },
      "source": [
        "gpu 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQqiNxQreNNY",
        "outputId": "ba8dbb0d-d767-4ce4-9b5b-f9f7a635ab5c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jul  3 03:45:50 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRweqOqFpabp"
      },
      "source": [
        "CNN 모델 구현 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY1PjM-MeZxP",
        "outputId": "76c936a7-b652-453a-b4bd-63715e1b63ac"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "'''\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "\n",
        "print(train_images[0])\n",
        "print(train_labels[0])\n",
        "'''\n",
        "\n",
        "#입력 데이터 전처리\n",
        "#train_images.max()\n",
        "train_images = train_images / 255.\n",
        "test_images = test_images / 255.\n",
        "\n",
        "train_images = train_images.reshape(60000, 28, 28, 1)\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "\n",
        "#라벨 데이터 전처리\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "\n",
        "#CNN 모델 구현\n",
        "\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=32, kernel_size=(3,3), \n",
        "    activation='relu',\n",
        "    input_shape=(28,28,1)))\n",
        "\n",
        "model.add(layers.MaxPool2D(\n",
        "    pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=64, kernel_size=(3,3), \n",
        "    activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPool2D(\n",
        "    pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=64, kernel_size=(3,3), \n",
        "    activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=64, activation='relu'))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "#model.summary()\n",
        "\n",
        "#모델의 학습설정\n",
        "#optimizer=['adam','adagrad','rmsprop']\n",
        "# \n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "model.fit(x=train_images, y=train_labels,\n",
        "          epochs=15, batch_size=256,\n",
        "          validation_split=0.2)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x=test_images, y=test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.4212 - accuracy: 0.8672 - val_loss: 0.2796 - val_accuracy: 0.9084\n",
            "Epoch 2/15\n",
            "188/188 [==============================] - 1s 8ms/step - loss: 0.0952 - accuracy: 0.9703 - val_loss: 0.0725 - val_accuracy: 0.9783\n",
            "Epoch 3/15\n",
            "188/188 [==============================] - 1s 8ms/step - loss: 0.0589 - accuracy: 0.9821 - val_loss: 0.0575 - val_accuracy: 0.9835\n",
            "Epoch 4/15\n",
            "188/188 [==============================] - 1s 8ms/step - loss: 0.0431 - accuracy: 0.9864 - val_loss: 0.0469 - val_accuracy: 0.9856\n",
            "Epoch 5/15\n",
            "188/188 [==============================] - 1s 8ms/step - loss: 0.0331 - accuracy: 0.9892 - val_loss: 0.0437 - val_accuracy: 0.9872\n",
            "Epoch 6/15\n",
            "188/188 [==============================] - 1s 8ms/step - loss: 0.0273 - accuracy: 0.9911 - val_loss: 0.0459 - val_accuracy: 0.9862\n",
            "Epoch 7/15\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0214 - accuracy: 0.9930 - val_loss: 0.0375 - val_accuracy: 0.9890\n",
            "Epoch 8/15\n",
            "188/188 [==============================] - 1s 8ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.0478 - val_accuracy: 0.9871\n",
            "Epoch 9/15\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 0.0411 - val_accuracy: 0.9877\n",
            "Epoch 10/15\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0379 - val_accuracy: 0.9904\n",
            "Epoch 11/15\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.0658 - val_accuracy: 0.9832\n",
            "Epoch 12/15\n",
            "188/188 [==============================] - 1s 8ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0417 - val_accuracy: 0.9908\n",
            "Epoch 13/15\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.0406 - val_accuracy: 0.9906\n",
            "Epoch 14/15\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0062 - accuracy: 0.9978 - val_loss: 0.0425 - val_accuracy: 0.9903\n",
            "Epoch 15/15\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.0054 - accuracy: 0.9981 - val_loss: 0.0472 - val_accuracy: 0.9909\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0304 - accuracy: 0.9922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQoODT8kweX2"
      },
      "source": [
        "CNN 활용한 f-mnist 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBHjRfDhweM4",
        "outputId": "37a0ce28-7e15-46d7-9f9f-24cf2cca2faa"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "\"\"\"* Step 1. Input tensor 와 Target tensor 준비(훈련데이터)\"\"\"\n",
        "\n",
        "# label 데이터의 각 value 에 해당하는 class name 정보\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. import 한 fashion_mnist API를 이용하여 fashion_mnist 데이터 셋을 다운로드 받으세요\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "\"\"\"* Step 2. 입력데이터의 전처리 \"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. 3차원 형태(batch, hight, width)의 train, test feature 데이터를 3차원(batch, hight, width, chanel(1))으로 변경 하세요\n",
        "# 2. feature 데이터를 [0, 1] 사이로 scailing을 수행하세요\n",
        "train_images = train_images.reshape(60000, 28, 28, 1)\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "\n",
        "train_images = train_images / 255.\n",
        "test_images = test_images / 255.\n",
        "\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. 1차원 형태의(batch, ) class id 인 train, test label 데이터를\n",
        "#    to_categorical API를 이용하여 one-hot-encoding 수행하여 2차원(batch, class_cnt) 으로 변경 하세요\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* Step3. CNN 모델 디자인\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. Sequential API를 이용하여 fashion_mnist 데이터 셋을 분석 하기 위한 CNN 모델을 디자인 하세요\n",
        "from tensorflow.keras import layers, models\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(\n",
        "    filters=32, kernel_size=(3,3), \n",
        "    activation='relu',\n",
        "    input_shape=(28,28,1)))\n",
        "\n",
        "model.add(layers.MaxPool2D(\n",
        "    pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=64, kernel_size=(3,3), \n",
        "    activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPool2D(\n",
        "    pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=64, kernel_size=(3,3), \n",
        "    activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=64, activation='relu'))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "\n",
        "\"\"\"* Step 4. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정하세요\n",
        "#   - optimizer\n",
        "#   - loss\n",
        "#   - metrics : 체점 기준인 accuracy 로 설정\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* Step 5. 모델에 input, target 데이터 연결 후 학습\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습하세요\n",
        "#   - fit 메서드의 verbose=2 로 설정 하세요\n",
        "model.fit(x=train_images, y=train_labels,\n",
        "          epochs=15, batch_size=256,\n",
        "          validation_split=0.2,\n",
        "          verbose=2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/15\n",
            "188/188 - 2s - loss: 0.8131 - accuracy: 0.6975 - val_loss: 0.5412 - val_accuracy: 0.7977\n",
            "Epoch 2/15\n",
            "188/188 - 1s - loss: 0.4756 - accuracy: 0.8232 - val_loss: 0.5265 - val_accuracy: 0.8127\n",
            "Epoch 3/15\n",
            "188/188 - 1s - loss: 0.3933 - accuracy: 0.8561 - val_loss: 0.4586 - val_accuracy: 0.8250\n",
            "Epoch 4/15\n",
            "188/188 - 1s - loss: 0.3445 - accuracy: 0.8727 - val_loss: 0.3528 - val_accuracy: 0.8695\n",
            "Epoch 5/15\n",
            "188/188 - 1s - loss: 0.3107 - accuracy: 0.8864 - val_loss: 0.3251 - val_accuracy: 0.8799\n",
            "Epoch 6/15\n",
            "188/188 - 1s - loss: 0.2866 - accuracy: 0.8945 - val_loss: 0.3084 - val_accuracy: 0.8857\n",
            "Epoch 7/15\n",
            "188/188 - 1s - loss: 0.2668 - accuracy: 0.9015 - val_loss: 0.2815 - val_accuracy: 0.8972\n",
            "Epoch 8/15\n",
            "188/188 - 1s - loss: 0.2489 - accuracy: 0.9094 - val_loss: 0.2757 - val_accuracy: 0.9001\n",
            "Epoch 9/15\n",
            "188/188 - 1s - loss: 0.2355 - accuracy: 0.9137 - val_loss: 0.2845 - val_accuracy: 0.8933\n",
            "Epoch 10/15\n",
            "188/188 - 1s - loss: 0.2225 - accuracy: 0.9185 - val_loss: 0.2873 - val_accuracy: 0.8948\n",
            "Epoch 11/15\n",
            "188/188 - 1s - loss: 0.2085 - accuracy: 0.9224 - val_loss: 0.2914 - val_accuracy: 0.8954\n",
            "Epoch 12/15\n",
            "188/188 - 1s - loss: 0.1977 - accuracy: 0.9269 - val_loss: 0.2809 - val_accuracy: 0.9028\n",
            "Epoch 13/15\n",
            "188/188 - 1s - loss: 0.1866 - accuracy: 0.9315 - val_loss: 0.2898 - val_accuracy: 0.8951\n",
            "Epoch 14/15\n",
            "188/188 - 1s - loss: 0.1774 - accuracy: 0.9341 - val_loss: 0.2489 - val_accuracy: 0.9118\n",
            "Epoch 15/15\n",
            "188/188 - 1s - loss: 0.1674 - accuracy: 0.9384 - val_loss: 0.2501 - val_accuracy: 0.9113\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcf801866d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBJTbJX_xLsP"
      },
      "source": [
        "CNN 활용한 cifar10 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dO_mvM0xLag",
        "outputId": "8dda0c27-d0a8-449d-862f-3e076e00ae25"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "#print(train_images.shape)\n",
        "#print(train_labels.shape)\n",
        "\n",
        "\n",
        "\n",
        "train_images = train_images / 255.\n",
        "test_images = test_images / 255.\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "model = models.Sequential()\n",
        "#(32,32,3)->(30,30,32)\n",
        "model.add(layers.Conv2D(\n",
        "    filters=32, kernel_size=(3,3), \n",
        "    activation='relu',\n",
        "    input_shape=(32,32,3)))\n",
        "#(30,30,64)->(15,15,32)\n",
        "model.add(layers.MaxPool2D(\n",
        "    pool_size=(2,2)))\n",
        "#(15,15,32)->(13,13,64)\n",
        "model.add(layers.Conv2D(\n",
        "    filters=64, kernel_size=(3,3), \n",
        "    activation='relu'))\n",
        "#(13,13,64)->(6,6,64)\n",
        "model.add(layers.MaxPool2D(\n",
        "    pool_size=(2,2)))\n",
        "#(6,6,64)->(4,4,64)\n",
        "model.add(layers.Conv2D(\n",
        "    filters=64, kernel_size=(3,3), \n",
        "    activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=64, activation='relu'))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#모델의 학습설정\n",
        "#optimizer=['adam','adagrad','rmsprop']\n",
        "# \n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "model.fit(x=train_images, y=train_labels,\n",
        "          epochs=20, batch_size=256,\n",
        "          validation_split=0.2)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x=test_images, y=test_labels)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_25 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 64)                65600     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 122,570\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "157/157 [==============================] - 3s 12ms/step - loss: 1.9334 - accuracy: 0.2996 - val_loss: 1.6432 - val_accuracy: 0.4093\n",
            "Epoch 2/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 1.5933 - accuracy: 0.4302 - val_loss: 1.5518 - val_accuracy: 0.4477\n",
            "Epoch 3/20\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 1.4310 - accuracy: 0.4905 - val_loss: 1.4756 - val_accuracy: 0.4814\n",
            "Epoch 4/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 1.3149 - accuracy: 0.5343 - val_loss: 1.2544 - val_accuracy: 0.5571\n",
            "Epoch 5/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 1.2229 - accuracy: 0.5692 - val_loss: 1.3418 - val_accuracy: 0.5346\n",
            "Epoch 6/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 1.1443 - accuracy: 0.5995 - val_loss: 1.1493 - val_accuracy: 0.5954\n",
            "Epoch 7/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 1.0810 - accuracy: 0.6217 - val_loss: 1.1286 - val_accuracy: 0.6113\n",
            "Epoch 8/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 1.0224 - accuracy: 0.6413 - val_loss: 1.2794 - val_accuracy: 0.5459\n",
            "Epoch 9/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.9700 - accuracy: 0.6616 - val_loss: 1.0772 - val_accuracy: 0.6309\n",
            "Epoch 10/20\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.9220 - accuracy: 0.6796 - val_loss: 1.0129 - val_accuracy: 0.6474\n",
            "Epoch 11/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.8820 - accuracy: 0.6931 - val_loss: 1.0134 - val_accuracy: 0.6436\n",
            "Epoch 12/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.8405 - accuracy: 0.7080 - val_loss: 1.0952 - val_accuracy: 0.6351\n",
            "Epoch 13/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.8064 - accuracy: 0.7207 - val_loss: 0.9479 - val_accuracy: 0.6719\n",
            "Epoch 14/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.7696 - accuracy: 0.7319 - val_loss: 1.0004 - val_accuracy: 0.6550\n",
            "Epoch 15/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.7431 - accuracy: 0.7412 - val_loss: 1.1829 - val_accuracy: 0.6190\n",
            "Epoch 16/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.7135 - accuracy: 0.7543 - val_loss: 1.0178 - val_accuracy: 0.6460\n",
            "Epoch 17/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6838 - accuracy: 0.7634 - val_loss: 1.0727 - val_accuracy: 0.6440\n",
            "Epoch 18/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6558 - accuracy: 0.7727 - val_loss: 0.9385 - val_accuracy: 0.6841\n",
            "Epoch 19/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6254 - accuracy: 0.7810 - val_loss: 0.9743 - val_accuracy: 0.6796\n",
            "Epoch 20/20\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.6042 - accuracy: 0.7902 - val_loss: 0.9315 - val_accuracy: 0.6953\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.9521 - accuracy: 0.6946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6BdJv40GMUi"
      },
      "source": [
        "CNN을 활용한 코드프레소-cifar10 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCB4dcPdDYex",
        "outputId": "0c31f523-9ad1-4aad-aca2-bec41b2583f6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "\n",
        "\n",
        "\"\"\"* Step 1. Input tensor 와 Target tensor 준비(훈련데이터)\"\"\"\n",
        "\n",
        "# label 데이터의 각 value 에 해당하는 class name 정보\n",
        "cifar_label_name=['apple', 'beaver', 'bottle', 'butterfly', 'castle',\n",
        "                  'clock', 'couch', 'leopard', 'rose', 'shark']\n",
        "\n",
        "# 데이터 다운로드 url 경로\n",
        "data_url = 'https://codepresso-online-platform-public.s3.ap-northeast-2.amazonaws.com/learning-resourse/Tensorflow+2.0+%EB%94%A5%EB%9F%AC%EB%8B%9D+%EC%99%84%EB%B2%BD+%EA%B0%80%EC%9D%B4%EB%93%9C/cifar-10-codepresso.npz'\n",
        "\n",
        "# requests 라이브러리를 이용해 데이터 다운로드\n",
        "response = requests.get(data_url)\n",
        "response.raise_for_status()\n",
        "\n",
        "# 다운로드 받은 데이터를 읽어 들여 Input tensor 와 Target tensor 준비\n",
        "with np.load(io.BytesIO(response.content)) as cifar_10_codepresso_data:\n",
        "  # 학습 이미지 데이터(np.ndarry, shape=(5000, 32, 32, 3))\n",
        "  train_images = cifar_10_codepresso_data['train_images']\n",
        "  # 학습 라벨 데이터(np.ndarry, shape=(5000, ))\n",
        "  train_labels = cifar_10_codepresso_data['train_labels']\n",
        "  \n",
        "  # 테스트 이미지 데이터(np.ndarry, shape=(1000, 32, 32, 3))\n",
        "  test_images = cifar_10_codepresso_data['test_images']\n",
        "  # 테스트 라벨 데이터(np.ndarry, shape=(1000, ))\n",
        "  test_labels = cifar_10_codepresso_data['test_labels']\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"* Step 2. 입력데이터의 전처리 \"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. feature 데이터를 [0, 1] 사이로 scailing을 수행하세요\n",
        "train_images = train_images / 255.\n",
        "test_images = test_images / 255.\n",
        "\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. 1차원 형태의(batch, ) class id 인 train, test label 데이터를\n",
        "#    to_categorical API를 이용하여 one-hot-encoding 수행하여 2차원(batch, class_cnt) 으로 변경 하세요\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "test_labels = to_categorical(test_labels)\n",
        "train_labels = to_categorical(train_labels)\n",
        "\n",
        "\n",
        "\"\"\"* Step3. CNN 모델 디자인\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. Sequential API를 이용하여 fashion_mnist 데이터 셋을 분석 하기 위한 CNN 모델을 디자인 하세요\n",
        "#   - 오버피팅 발생 시 classification module에 layers.Dropout 레이어를 추가 하여 성능을 향상시켜 보세요\n",
        "drop_rate=0.3\n",
        "\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=32, kernel_size=(3,3), \n",
        "    activation='relu',\n",
        "    input_shape=(32,32,3)))\n",
        "\n",
        "model.add(layers.Dropout(drop_rate))\n",
        "\n",
        "model.add(layers.MaxPool2D(\n",
        "    pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Dropout(drop_rate))\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=64, kernel_size=(3,3), \n",
        "    activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPool2D(\n",
        "    pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Dropout(drop_rate))\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=64, kernel_size=(3,3), \n",
        "    activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(units=64, activation='relu'))\n",
        "model.add(layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "\n",
        "\"\"\"* Step 4. 모델의 학습 정보 설정\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 compile 메서드를 이용하여 학습을 위한 정보들을 설정하세요\n",
        "#   - optimizer\n",
        "#   - loss\n",
        "#   - metrics : 체점 기준인 accuracy 로 설정\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\"\"\"* Step 5. 모델에 input, target 데이터 연결 후 학습\"\"\"\n",
        "\n",
        "# 수강생 작성 코드\n",
        "# 1. tf.keras.Model 객체의 fit 메서드를 이용하여 모델을 학습하세요\n",
        "#   - fit 메서드의 verbose=2 로 설정 하세요\n",
        "model.fit(x=train_images, y=train_labels,\n",
        "          epochs=40, batch_size=256,\n",
        "          validation_split=0.2,\n",
        "          verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "16/16 - 1s - loss: 2.2285 - accuracy: 0.1805 - val_loss: 2.1648 - val_accuracy: 0.2610\n",
            "Epoch 2/40\n",
            "16/16 - 0s - loss: 1.9613 - accuracy: 0.2918 - val_loss: 1.9859 - val_accuracy: 0.2940\n",
            "Epoch 3/40\n",
            "16/16 - 0s - loss: 1.8421 - accuracy: 0.3453 - val_loss: 1.8908 - val_accuracy: 0.3480\n",
            "Epoch 4/40\n",
            "16/16 - 0s - loss: 1.7327 - accuracy: 0.3915 - val_loss: 1.9576 - val_accuracy: 0.2670\n",
            "Epoch 5/40\n",
            "16/16 - 0s - loss: 1.7085 - accuracy: 0.3927 - val_loss: 1.8058 - val_accuracy: 0.4120\n",
            "Epoch 6/40\n",
            "16/16 - 0s - loss: 1.5925 - accuracy: 0.4340 - val_loss: 1.7392 - val_accuracy: 0.4360\n",
            "Epoch 7/40\n",
            "16/16 - 0s - loss: 1.5727 - accuracy: 0.4515 - val_loss: 1.7386 - val_accuracy: 0.4610\n",
            "Epoch 8/40\n",
            "16/16 - 0s - loss: 1.5055 - accuracy: 0.4720 - val_loss: 1.6378 - val_accuracy: 0.4460\n",
            "Epoch 9/40\n",
            "16/16 - 0s - loss: 1.4718 - accuracy: 0.4857 - val_loss: 1.6014 - val_accuracy: 0.5020\n",
            "Epoch 10/40\n",
            "16/16 - 0s - loss: 1.4125 - accuracy: 0.5070 - val_loss: 1.5497 - val_accuracy: 0.5110\n",
            "Epoch 11/40\n",
            "16/16 - 0s - loss: 1.3641 - accuracy: 0.5305 - val_loss: 1.5462 - val_accuracy: 0.4710\n",
            "Epoch 12/40\n",
            "16/16 - 0s - loss: 1.3543 - accuracy: 0.5260 - val_loss: 1.4611 - val_accuracy: 0.5120\n",
            "Epoch 13/40\n",
            "16/16 - 0s - loss: 1.3173 - accuracy: 0.5433 - val_loss: 1.4279 - val_accuracy: 0.5500\n",
            "Epoch 14/40\n",
            "16/16 - 0s - loss: 1.2646 - accuracy: 0.5602 - val_loss: 1.4042 - val_accuracy: 0.5780\n",
            "Epoch 15/40\n",
            "16/16 - 0s - loss: 1.2330 - accuracy: 0.5805 - val_loss: 1.3822 - val_accuracy: 0.5430\n",
            "Epoch 16/40\n",
            "16/16 - 0s - loss: 1.2329 - accuracy: 0.5665 - val_loss: 1.3169 - val_accuracy: 0.5900\n",
            "Epoch 17/40\n",
            "16/16 - 0s - loss: 1.1621 - accuracy: 0.6070 - val_loss: 1.2556 - val_accuracy: 0.5890\n",
            "Epoch 18/40\n",
            "16/16 - 0s - loss: 1.1755 - accuracy: 0.5932 - val_loss: 1.3186 - val_accuracy: 0.5740\n",
            "Epoch 19/40\n",
            "16/16 - 0s - loss: 1.1339 - accuracy: 0.6155 - val_loss: 1.2995 - val_accuracy: 0.5550\n",
            "Epoch 20/40\n",
            "16/16 - 0s - loss: 1.1030 - accuracy: 0.6227 - val_loss: 1.2414 - val_accuracy: 0.5820\n",
            "Epoch 21/40\n",
            "16/16 - 0s - loss: 1.1055 - accuracy: 0.6212 - val_loss: 1.1862 - val_accuracy: 0.6230\n",
            "Epoch 22/40\n",
            "16/16 - 0s - loss: 1.0748 - accuracy: 0.6355 - val_loss: 1.2824 - val_accuracy: 0.5530\n",
            "Epoch 23/40\n",
            "16/16 - 0s - loss: 1.0633 - accuracy: 0.6305 - val_loss: 1.1944 - val_accuracy: 0.6080\n",
            "Epoch 24/40\n",
            "16/16 - 0s - loss: 1.0181 - accuracy: 0.6485 - val_loss: 1.1791 - val_accuracy: 0.6240\n",
            "Epoch 25/40\n",
            "16/16 - 0s - loss: 1.0067 - accuracy: 0.6460 - val_loss: 1.1621 - val_accuracy: 0.6440\n",
            "Epoch 26/40\n",
            "16/16 - 0s - loss: 0.9843 - accuracy: 0.6603 - val_loss: 1.1126 - val_accuracy: 0.6640\n",
            "Epoch 27/40\n",
            "16/16 - 0s - loss: 0.9625 - accuracy: 0.6693 - val_loss: 1.1406 - val_accuracy: 0.6310\n",
            "Epoch 28/40\n",
            "16/16 - 0s - loss: 0.9513 - accuracy: 0.6725 - val_loss: 1.1470 - val_accuracy: 0.6600\n",
            "Epoch 29/40\n",
            "16/16 - 0s - loss: 0.9225 - accuracy: 0.6930 - val_loss: 1.1073 - val_accuracy: 0.6350\n",
            "Epoch 30/40\n",
            "16/16 - 0s - loss: 0.9190 - accuracy: 0.6900 - val_loss: 1.2044 - val_accuracy: 0.5750\n",
            "Epoch 31/40\n",
            "16/16 - 0s - loss: 0.8943 - accuracy: 0.7000 - val_loss: 1.3350 - val_accuracy: 0.5320\n",
            "Epoch 32/40\n",
            "16/16 - 0s - loss: 0.8876 - accuracy: 0.6917 - val_loss: 1.0758 - val_accuracy: 0.6580\n",
            "Epoch 33/40\n",
            "16/16 - 0s - loss: 0.8517 - accuracy: 0.7028 - val_loss: 1.2630 - val_accuracy: 0.5750\n",
            "Epoch 34/40\n",
            "16/16 - 0s - loss: 0.8545 - accuracy: 0.7103 - val_loss: 1.0607 - val_accuracy: 0.6440\n",
            "Epoch 35/40\n",
            "16/16 - 0s - loss: 0.8419 - accuracy: 0.7092 - val_loss: 1.1156 - val_accuracy: 0.6370\n",
            "Epoch 36/40\n",
            "16/16 - 0s - loss: 0.8172 - accuracy: 0.7120 - val_loss: 1.0127 - val_accuracy: 0.7030\n",
            "Epoch 37/40\n",
            "16/16 - 0s - loss: 0.8180 - accuracy: 0.7200 - val_loss: 1.0856 - val_accuracy: 0.6250\n",
            "Epoch 38/40\n",
            "16/16 - 0s - loss: 0.8021 - accuracy: 0.7358 - val_loss: 1.2338 - val_accuracy: 0.5930\n",
            "Epoch 39/40\n",
            "16/16 - 0s - loss: 0.7632 - accuracy: 0.7358 - val_loss: 1.0750 - val_accuracy: 0.6300\n",
            "Epoch 40/40\n",
            "16/16 - 0s - loss: 0.7537 - accuracy: 0.7418 - val_loss: 0.9630 - val_accuracy: 0.6970\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcf6e2da6d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}